import streamlit as st
import torch
from PIL import Image
from transformers import (
    AutoProcessor, AutoModel, 
    CLIPProcessor, CLIPModel, 
    AutoTokenizer, SiglipProcessor, SiglipModel
)
import json
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ---------------------------------------------------------------------------------------------------
# 0. [í•µì‹¬] ëª¨ë¸ ì„¤ì • (AltCLIP, KoCLIP ë“± ë¦¬ëª¨íŠ¸ ì½”ë“œ í•„ìš”í•œ ëª¨ë¸ ì„¤ì • ê°•í™”)
# ---------------------------------------------------------------------------------------------------
MODELS_CONFIG = [
    {
        "name": "KoCLIP (Ours)", 
        "id": "koclip/koclip-base-pt", 
        "type": "koclip",
        "desc": "í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (ì„ ì • ëª¨ë¸)"
    },
    {
        "name": "OpenAI CLIP (Base)", 
        "id": "openai/clip-vit-base-patch32", 
        "type": "clip_std",
        "desc": "ê¸€ë¡œë²Œ ìŠ¤íƒ ë‹¤ë“œ (ì˜ì–´ ê¸°ë°˜)"
    },
    {
        "name": "AltCLIP (Multilingual)", 
        "id": "BAAI/AltCLIP", 
        "type": "clip_std", 
        "desc": "ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ (ë¹„êµêµ°)"
    },
    # {
    #     "name": "Google SigLIP (SoTA)", 
    #     "id": "google/siglip-base-patch16-224", 
    #     "type": "siglip",
    #     "desc": "êµ¬ê¸€ì˜ ìµœì‹  ëª¨ë¸ (ì„±ëŠ¥ ë§¤ìš° ë†’ìŒ)"
    # },
]

# ------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •
# ------------------------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
st.set_page_config(page_title="LMM Model Arena", layout="wide")

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows ê¹¨ì§ ë°©ì§€)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

st.title("ğŸŸï¸ LMM ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì•„ë ˆë‚˜")
st.markdown(f"""
**ì‹¤í–‰ ìƒíƒœ:** `{device.upper()}` í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘
* **KoCLIP / AltCLIP ë¡œë“œ íŒ:** ë³´ì•ˆ ê²½ê³ ê°€ ëœ¨ë©´ `transformers` ë²„ì „ì„ 4.46.3ìœ¼ë¡œ ë‚®ì¶°ì£¼ì„¸ìš”.
""")

# ------------------------------------------------
# 2. ë™ì  ëª¨ë¸ ë¡œë” (ì•ˆì •ì„± ê°•í™” ë²„ì „)
# ------------------------------------------------
@st.cache_resource
def load_all_models():
    loaded_models = {}
    
    for config in MODELS_CONFIG:
        model_name = config['name']
        model_id = config['id']
        m_type = config['type']
        
        print(f"ğŸš€ ë¡œë”© ì‹œì‘: {model_name}...") 
        
        try:
            # --- A. KoCLIP ë¡œë“œ ---
            if m_type == 'koclip':
                # KoCLIPì€ koclip/koclip-base-pt ê²½ë¡œì—ì„œ ë°”ë¡œ ë¡œë“œ
                model = AutoModel.from_pretrained(
                    model_id, 
                    trust_remote_code=True # í•„ìˆ˜: ì™¸ë¶€ ì½”ë“œ í—ˆìš©
                ).to(device)
                
                tokenizer = AutoTokenizer.from_pretrained(
                    model_id,
                    trust_remote_code=True
                )
                # KoCLIPì€ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ OpenAI CLIPì˜ ì „ì²˜ë¦¬ê¸°(Processor)ë¥¼ ë¹Œë ¤ ì”€
                processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                
                loaded_models[model_name] = {
                    "model": model, "tokenizer": tokenizer, "processor": processor, "type": m_type
                }
                
            # --- B. SigLIP ë¡œë“œ ---
            elif m_type == 'siglip':
                model = SiglipModel.from_pretrained(model_id).to(device)
                processor = SiglipProcessor.from_pretrained(model_id)
                loaded_models[model_name] = {
                    "model": model, "processor": processor, "type": m_type
                }
                
            # --- C. CLIP / AltCLIP (Standard) ---
            else:
                # AltCLIP ë“±ì€ trust_remote_code=Trueê°€ ìˆì–´ì•¼ ì•ˆì „í•˜ê²Œ ë¡œë“œë¨
                model = AutoModel.from_pretrained(
                    model_id, 
                    trust_remote_code=True 
                ).to(device)
                
                try:
                    processor = AutoProcessor.from_pretrained(
                        model_id, 
                        trust_remote_code=True
                    )
                except:
                    # ë§Œì•½ AutoProcessorê°€ ì‹¤íŒ¨í•˜ë©´ CLIPProcessorë¡œ ì‹œë„
                    processor = CLIPProcessor.from_pretrained(model_id)

                loaded_models[model_name] = {
                    "model": model, "processor": processor, "type": "auto"
                }
            
            print(f"âœ… {model_name} ë¡œë“œ ì„±ê³µ")

        except Exception as e:
            print(f"âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
            # Streamlit í™”ë©´ì—ë„ ì—ëŸ¬ ë„ì›Œì£¼ê¸° (ë””ë²„ê¹…ìš©)
            st.error(f"âš ï¸ **{model_name}** ë¡œë“œ ì‹¤íŒ¨! \nì—ëŸ¬ ë‚´ìš©: {e}")
            continue
            
    return loaded_models

# ------------------------------------------------
# 3. í†µí•© ì¶”ë¡  ì—”ì§„
# ------------------------------------------------
def get_similarity_score(model_pack, image, text):
    model = model_pack['model']
    m_type = model_pack['type']
    
    try:
        with torch.no_grad():
            # --- A. KoCLIP ---
            if m_type == 'koclip':
                processor = model_pack['processor']
                tokenizer = model_pack['tokenizer']
                
                # ì´ë¯¸ì§€ ì²˜ë¦¬
                img_inputs = processor(images=image, return_tensors="pt").to(device)
                # í…ìŠ¤íŠ¸ ì²˜ë¦¬
                txt_inputs = tokenizer([text], padding=True, return_tensors="pt").to(device)
                
                img_feat = model.get_image_features(**img_inputs)
                txt_feat = model.get_text_features(**txt_inputs)
                
                # ì •ê·œí™” ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                img_feat /= img_feat.norm(dim=-1, keepdim=True)
                txt_feat /= txt_feat.norm(dim=-1, keepdim=True)
                return (img_feat @ txt_feat.T).item()

            # --- B. Google SigLIP ---
            elif m_type == 'siglip':
                processor = model_pack['processor']
                inputs = processor(text=[text], images=image, return_tensors="pt", padding="max_length").to(device)
                outputs = model(**inputs)
                
                logits = outputs.logits_per_image.item()
                # SigLIP ìŠ¤ì¼€ì¼ë§ (Logitsê°€ í¼)
                return max(0, logits) / 10.0 

            # --- C. Standard CLIP / AltCLIP ---
            else:
                processor = model_pack['processor']
                # AltCLIPì€ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ truncation ì˜µì…˜ ì¶”ê°€
                inputs = processor(
                    text=[text], 
                    images=image, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=77 
                ).to(device)
                
                outputs = model(**inputs)
                
                # CLIP ê³„ì—´ì€ ë³´í†µ Logit Scaleì´ 100ì´ë¯€ë¡œ 100ìœ¼ë¡œ ë‚˜ëˆ ì„œ 0~1 ì‚¬ì´ë¡œ ë§ì¶¤
                return outputs.logits_per_image.item() / 100.0
                
    except Exception as e:
        print(f"Inference Error ({m_type}): {e}")
        return 0.0

# ------------------------------------------------
# 4. ë©”ì¸ UI ë¡œì§
# ------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    uploaded_file = st.file_uploader("ë°ì´í„°ì…‹ (JSON)", type=['json'])
    default_path = os.path.join("data", "images")
    image_folder = st.text_input("ì´ë¯¸ì§€ ê²½ë¡œ", value=default_path)
    
    st.divider()
    st.write("ğŸ“‹ **ì°¸ì „ ëª¨ë¸ ëª©ë¡**")
    for conf in MODELS_CONFIG:
        st.caption(f"- {conf['name']}")

if uploaded_file and image_folder:
    data = json.load(uploaded_file)
    
    if st.button("ğŸ”¥ ì•„ë ˆë‚˜ ë°°í‹€ ì‹œì‘ (Run Benchmark)"):
        
        loaded_models = load_all_models()
        
        if not loaded_models:
            st.error("âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ 'pip install transformers==4.46.3'ì„ ì‹¤í–‰í•´ë³´ì„¸ìš”.")
            st.stop()
            
        st.success(f"ì´ {len(loaded_models)}ê°œì˜ ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # --- ë””ë²„ê¹…ìš© ë¡œê·¸ì°½ ---
        log_expander = st.expander("ğŸ” ì§„í–‰ ë¡œê·¸ í™•ì¸", expanded=True)
        
        for i, item in enumerate(data):
            img_name = item.get('image_filename') or item.get('image_file') or item.get('filename')
            caption = item.get('caption') or item.get('description') or item.get('text')
            
            if not img_name: continue
            if not caption: caption = "unknown"
            
            img_path = os.path.join(image_folder, img_name)
            
            if not os.path.exists(img_path):
                if i < 5: log_expander.warning(f"ì´ë¯¸ì§€ ëª» ì°¾ìŒ: {img_name}")
                continue
            
            try:
                image = Image.open(img_path).convert("RGB")
            except:
                continue
            
            row_data = {"Index": i}
            
            for m_name, m_pack in loaded_models.items():
                score = get_similarity_score(m_pack, image, caption)
                row_data[m_name] = score
            
            results.append(row_data)
            
            progress_bar.progress((i + 1) / len(data))
            if i % 10 == 0:
                status_text.text(f"ì²˜ë¦¬ ì¤‘... {i+1}/{len(data)}")
        
        # --- ê²°ê³¼ ì²˜ë¦¬ ---
        if results:
            df = pd.DataFrame(results)
            st.divider()
            st.subheader("ğŸ† ìµœì¢… ìŠ¤ì½”ì–´ë³´ë“œ")
            
            # ìˆ«ì ë°ì´í„°ë§Œ ê³¨ë¼ì„œ í‰ê·  ë‚´ê¸°
            numeric_cols = [col for col in df.columns if col not in ['Index']]
            means = df[numeric_cols].mean().sort_values(ascending=False)
            
            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            fig, ax = plt.subplots(figsize=(10, 5))
            # KoCLIP ê°•ì¡°ìƒ‰
            colors = ['#FF4B4B' if 'KoCLIP' in idx else '#A9A9A9' for idx in means.index]
            sns.barplot(x=means.index, y=means.values, palette=colors, ax=ax)
            
            ax.set_title("Image-Text Alignment Score (Cosine Similarity)", fontsize=14, fontweight='bold')
            ax.set_ylabel("í‰ê·  ìœ ì‚¬ë„ (0~1)")
            ax.set_ylim(0, 0.6) # Yì¶• ê³ ì • (ë¹„êµ í¸í•˜ê²Œ)
            
            # ë§‰ëŒ€ ìœ„ì— ì ìˆ˜ í‘œì‹œ
            for p in ax.patches:
                ax.annotate(f'{p.get_height():.4f}', 
                           (p.get_x() + p.get_width() / 2., p.get_height()), 
                           ha='center', va='bottom', fontsize=10, fontweight='bold')
            
            st.pyplot(fig)
            
            # ë¶„ì„ ë©˜íŠ¸
            winner = means.idxmax()
            st.success(f"ğŸ¥‡ **ìµœì¢… ìŠ¹ì:** {winner}")
            st.info(f"""
            **[ê²°ê³¼ í•´ì„]**
            * **{winner}** ëª¨ë¸ì´ í‰ê·  ìœ ì‚¬ë„ **{means.max():.4f}**ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.
            * ì´ëŠ” í…ìŠ¤íŠ¸ ì„¤ëª…ê³¼ ì´ë¯¸ì§€ ê°„ì˜ ì˜ë¯¸ì  ì—°ê²°(Alignment)ì´ ê°€ì¥ ê°•ë ¥í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            * 0ì (0.0000)ì´ ë‚˜ì˜¨ ëª¨ë¸ì´ ìˆë‹¤ë©´ ë¡œë“œ ì‹¤íŒ¨ì´ë¯€ë¡œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.
            """)
        else:
            st.error("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")