import streamlit as st
import torch
from PIL import Image
from transformers import (
    AutoProcessor, AutoModel, 
    CLIPProcessor, CLIPModel, 
    AutoTokenizer, SiglipProcessor, SiglipModel
)
import json
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ---------------------------------------------------------------------------------------------------
# 0. [í•µì‹¬] ëª¨ë¸ í™•ì¥ ì„¤ì • (ë¹„êµí•˜ê³  ì‹¶ì€ê±° ì¶”ê°€í•˜ë©´ ë¨) ëŒë ¤ë³´ë‹ˆ 4ê°œì´ìƒì´ë©´ ì»´í„°ì•„íŒŒí•¨ ëŒë¦´ê²ƒë“¤ë§Œ ì£¼ì„í’€ì–´ì„œë¹„êµã„± 
# ---------------------------------------------------------------------------------------------------
MODELS_CONFIG = [
    {
        "name": "KoCLIP (Ours)", 
        "id": "koclip/koclip-base-pt", 
        "type": "koclip",
        "desc": "í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (ì„ ì • ëª¨ë¸)"
    },
    {
        "name": "OpenAI CLIP (Base)", 
        "id": "openai/clip-vit-base-patch32", 
        "type": "clip_std",
        "desc": "ê¸€ë¡œë²Œ ìŠ¤íƒ ë‹¤ë“œ (ì˜ì–´ ê¸°ë°˜)"
    },
    {
        "name": "Google SigLIP (SoTA)", 
        "id": "google/siglip-base-patch16-224", 
        "type": "siglip",
        "desc": "êµ¬ê¸€ì˜ ìµœì‹  ëª¨ë¸ (ì„±ëŠ¥ ë§¤ìš° ë†’ìŒ)"
    },
    {
        "name": "AltCLIP (Multilingual)", 
        "id": "BAAI/AltCLIP", 
        "type": "clip_std", 
        "desc": "ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ (ë¹„êµêµ°)"
    },
    # {
    #     "name": "Fashion-CLIP", 
    #     "id": "patrickjohncyh/fashion-clip", 
    #     "type": "clip_std",
    #     "desc": " íŒ¨ì…˜ ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ (ì‡¼í•‘ëª° ìµœì )"
    # },
    # {
    #     "name": "AltCLIP (Multilingual)", 
    #     "id": "BAAI/AltCLIP", 
    #     "type": "clip_std",
    #     "desc": "ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´ ê°€ëŠ¥, ë¬´ê±°ì›€)"
    # },
    # {
    #     "name": "MetaCLIP (Facebook)", 
    #     "id": "facebook/metaclip-b32-400m", 
    #     "type": "clip_std",
    #     "desc": "ë©”íƒ€(í˜ì´ìŠ¤ë¶)ì˜ ê³ ì„±ëŠ¥ CLIP"
    # },
    # {
    #     "name": "LAION-2B (Open Source)", 
    #     "id": "laion/CLIP-ViT-B-32-laion2B-s34B-b79K", 
    #     "type": "clip_std",
    #     "desc": "ì˜¤í”ˆì†ŒìŠ¤ ë°ì´í„° 20ì–µê°œë¡œ í•™ìŠµ"
    # },
    # {
    #     "name": "DFN-CLIP (Apple)", 
    #     "id": "apple/DFN5B-CLIP-ViT-H-14-378", 
    #     "type": "clip_std",
    #     "desc": "ì• í”Œì˜ ê³ í’ˆì§ˆ ë°ì´í„° í•™ìŠµ (ì´ˆëŒ€í˜• ëª¨ë¸)"
    # }
]

# ------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •
# ------------------------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
st.set_page_config(page_title="LMM Model Arena", layout="wide")

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows ê¹¨ì§ ë°©ì§€)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

st.title("ğŸŸï¸ LMM ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì•„ë ˆë‚˜")
st.markdown(f"""
**ì‹¤í—˜ ëª©ì :** ìœ ëª…í•œ ê¸€ë¡œë²Œ LMM ëª¨ë¸ë“¤ê³¼ **KoCLIP**ì„ ë™ì¼í•œ ì¡°ê±´ì—ì„œ ê²½ìŸì‹œì¼œ, 
í•œêµ­ì–´ ì‡¼í•‘ëª° ê²€ìƒ‰ í™˜ê²½ì—ì„œì˜ **ì í•©ì„±(Accuracy)**ê³¼ **íš¨ìœ¨ì„±(Speed)**ì„ ì¦ëª…í•©ë‹ˆë‹¤.
* **ì‹¤í–‰ í™˜ê²½:** {device.upper()}
""")

# ------------------------------------------------
# 2. ë™ì  ëª¨ë¸ ë¡œë”
# ------------------------------------------------
@st.cache_resource
def load_all_models():
    loaded_models = {}
    
    for config in MODELS_CONFIG:
        model_name = config['name']
        model_id = config['id']
        m_type = config['type']
        
        print(f"ğŸš€ ë¡œë”© ì‹œì‘: {model_name}") 
        
        try:
            if m_type == 'koclip':
                model = AutoModel.from_pretrained(model_id).to(device)
                tokenizer = AutoTokenizer.from_pretrained(model_id)
                processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                loaded_models[model_name] = {
                    "model": model, "tokenizer": tokenizer, "processor": processor, "type": m_type
                }
                
            elif m_type == 'siglip':
                model = SiglipModel.from_pretrained(model_id).to(device)
                processor = SiglipProcessor.from_pretrained(model_id)
                loaded_models[model_name] = {
                    "model": model, "processor": processor, "type": m_type
                }
                
            else:
                # ì¼ë°˜ì ì¸ CLIP ê³„ì—´
                model = AutoModel.from_pretrained(model_id, trust_remote_code=True).to(device)
                processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
                loaded_models[model_name] = {
                    "model": model, "processor": processor, "type": "auto"
                }
            
            print(f"âœ… {model_name} ë¡œë“œ ì„±ê³µ")

        except Exception as e:
            
            print(f"âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue
            
    return loaded_models

# ------------------------------------------------
# 3. í†µí•© ì¶”ë¡  ì—”ì§„
# ------------------------------------------------
def get_similarity_score(model_pack, image, text):
    model = model_pack['model']
    m_type = model_pack['type']
    
    try:
        with torch.no_grad():
            # --- A. KoCLIP ---
            if m_type == 'koclip':
                processor = model_pack['processor']
                tokenizer = model_pack['tokenizer']
                
                img_inputs = processor(images=image, return_tensors="pt").to(device)
                txt_inputs = tokenizer([text], padding=True, return_tensors="pt").to(device)
                
                img_feat = model.get_image_features(**img_inputs)
                txt_feat = model.get_text_features(**txt_inputs)
                
                img_feat /= img_feat.norm(dim=-1, keepdim=True)
                txt_feat /= txt_feat.norm(dim=-1, keepdim=True)
                return (img_feat @ txt_feat.T).item()

            # --- B. Google SigLIP ---
            elif m_type == 'siglip':
                processor = model_pack['processor']
                inputs = processor(text=[text], images=image, return_tensors="pt", padding="max_length").to(device)
                outputs = model(**inputs)
                # SigLIPì€ ê°’ì´ í¼ -> 0~1 ì‚¬ì´ë¡œ ëŒ€ëµì  ìŠ¤ì¼€ì¼ë§ (ë¹„êµìš©)
                logits = outputs.logits_per_image.item()
                return max(0, logits) / 10.0 

            # --- C. Standard CLIP ---
            else:
                processor = model_pack['processor']
                inputs = processor(text=[text], images=image, return_tensors="pt", padding=True).to(device)
                outputs = model(**inputs)
                return outputs.logits_per_image.item() / 100.0
                
    except Exception:
        return 0.0

# ------------------------------------------------
# 4. ë©”ì¸ UI
# ------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    uploaded_file = st.file_uploader("ë°ì´í„°ì…‹ (JSON)", type=['json'])
    
    default_path = os.path.join("data", "images")
    image_folder = st.text_input("ì´ë¯¸ì§€ ê²½ë¡œ", value=default_path)
    
    st.divider()
    st.write("ğŸ“‹ **ë¹„êµ ëª¨ë¸ ëª©ë¡**")
    for conf in MODELS_CONFIG:
        st.caption(f"- {conf['name']}")

if uploaded_file and image_folder:
    data = json.load(uploaded_file)
    
    if st.button("ğŸ”¥ ì•„ë ˆë‚˜ ë°°í‹€ ì‹œì‘ (Run Benchmark)"):
        
        loaded_models = load_all_models()
        
        # ë¡œë“œëœ ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì—ëŸ¬ ì¶œë ¥í•˜ê³  ë©ˆì¶¤
        if not loaded_models:
            st.error("âŒ ë¡œë“œëœ ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê±°ë‚˜ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
            st.stop()
            
        st.success(f"ì´ {len(loaded_models)}ê°œì˜ ëª¨ë¸ì´ ì°¸ì „í–ˆìŠµë‹ˆë‹¤! ì‹¤í—˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        start_time = time.time()
        
        # --- [ë””ë²„ê¹…] ë¡œê·¸ì°½ ìƒì„± ---
        log_container = st.expander("ğŸ” ë””ë²„ê¹… ë¡œê·¸ (ë¬¸ì œê°€ ìƒê¸°ë©´ ì—¬ê¸°ë¥¼ í´ë¦­í•˜ì„¸ìš”)", expanded=True)
        
        for i, item in enumerate(data):
            # [ìˆ˜ì • í¬ì¸íŠ¸] JSON íŒŒì¼ì— ìˆëŠ” 'image_filename' í‚¤ë¥¼ ì œì¼ ë¨¼ì € ì°¾ë„ë¡ ë³€ê²½!
            img_name = item.get('image_filename') or item.get('image_file') or item.get('filename')
            caption = item.get('caption') or item.get('description') or item.get('text')
            
            # [ë””ë²„ê·¸] ì´ì œ íŒŒì¼ëª…ì„ ì œëŒ€ë¡œ ì°¾ëŠ”ì§€ í™•ì¸
            if i < 3:
                log_container.write(f"[{i}ë²ˆ ë°ì´í„°] ì°¾ì€ íŒŒì¼ëª…: {img_name}")

            if not img_name:
                # ì—¬ì „íˆ ëª» ì°¾ìœ¼ë©´ ì—ëŸ¬ ì¶œë ¥
                if i < 5: log_container.error(f"[{i}ë²ˆ] ì—¬ì „íˆ íŒŒì¼ëª…ì„ ëª» ì°¾ìŒ. í‚¤ ëª©ë¡: {list(item.keys())}")
                continue
            
            if not caption: caption = "unknown"
            
            # 2. ê²½ë¡œ í™•ì¸ ë° ì´ë¯¸ì§€ ë¡œë“œ
            img_path = os.path.join(image_folder, img_name)
            
            # íŒŒì¼ì´ ì§„ì§œ ìˆëŠ”ì§€ í™•ì¸
            if not os.path.exists(img_path):
                if i < 5: 
                    log_container.error(f"âŒ íŒŒì¼ì´ í´ë”ì— ì—†ìŒ: {img_path}")
                    log_container.info(f"í´ë” ê²½ë¡œ '{image_folder}' ì•ˆì— '{img_name}' íŒŒì¼ì´ ë“¤ì–´ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
                continue
            
            try:
                image = Image.open(img_path).convert("RGB")
            except Exception as e:
                if i < 5: log_container.error(f"ì´ë¯¸ì§€ ê¹¨ì§ ({img_name}): {e}")
                continue
            
            row_data = {"Index": i, "Caption": caption}
            
            # ë¡œë“œì— ì„±ê³µí•œ ëª¨ë¸ë“¤ë§Œ ëŒë¦¼
            for m_name, m_pack in loaded_models.items():
                score = get_similarity_score(m_pack, image, caption)
                row_data[m_name] = score
            
            results.append(row_data)
            
            progress = (i + 1) / len(data)
            progress_bar.progress(progress)
            if i % 5 == 0:
                status_text.text(f"Processing {i+1}/{len(data)}...")
        
        total_time = time.time() - start_time
        
        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ì¶œë ¥í•˜ê³  ë©ˆì¶¤
        if not results:
            st.error("ğŸš¨ ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ 'ë””ë²„ê¹… ë¡œê·¸'ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
            st.warning("ê°€ì¥ í”í•œ ì›ì¸: ì´ë¯¸ì§€ í´ë” ê²½ë¡œê°€ í‹€ë ¸ê±°ë‚˜, JSON íŒŒì¼ ì•ˆì˜ íŒŒì¼ëª…ê³¼ ì‹¤ì œ íŒŒì¼ëª…ì´ ë‹¤ë¦…ë‹ˆë‹¤.")
            st.stop()

        df = pd.DataFrame(results)
        
        # ------------------------------------------------
        # 5. ê²°ê³¼ ì‹œê°í™”
        # ------------------------------------------------
        st.divider()
        st.subheader("ğŸ† ìµœì¢… ìŠ¤ì½”ì–´ë³´ë“œ")
        
        # ë¡œë“œëœ ëª¨ë¸ ì»¬ëŸ¼ë§Œ ì„ íƒí•´ì„œ í‰ê·  ê³„ì‚°
        valid_model_cols = [name for name in loaded_models.keys() if name in df.columns]
        
        if not valid_model_cols:
            st.error("ê²°ê³¼ë¥¼ ê³„ì‚°í•  ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        means = df[valid_model_cols].mean().sort_values(ascending=False)
        
        # ê·¸ë˜í”„
        fig, ax = plt.subplots(figsize=(12, 6))
        colors = ['#FF4B4B' if 'KoCLIP' in name else '#A9A9A9' for name in means.index]
        
        sns.barplot(x=means.index, y=means.values, palette=colors, ax=ax)
        ax.set_title("ëª¨ë¸ë³„ í‰ê·  ì˜ë¯¸ ì´í•´ë„ (Semantic Accuracy)", fontsize=16, fontweight='bold')
        ax.set_ylabel("ìœ ì‚¬ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        
        for p in ax.patches:
            ax.annotate(f'{p.get_height():.4f}', 
                        (p.get_x() + p.get_width() / 2., p.get_height()), 
                        ha='center', va='bottom', fontsize=11, fontweight='bold')
            
        st.pyplot(fig)
        
        # ìŠ¹ì ê²°ì • ë¡œì§
        if not means.empty:
            winner = means.idxmax()
            st.success(f"ğŸ‰ **ìµœì¢… ìŠ¹ì:** {winner}")
            
            st.info(f"""
            **[ê²°ê³¼ ë¶„ì„]**
            * **{winner}** ëª¨ë¸ì´ í˜„ì¬ ë°ì´í„°ì…‹ì—ì„œ ê°€ì¥ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
            * í•œêµ­ì–´ ì‡¼í•‘ ë°ì´í„° íŠ¹ì„±ìƒ í•œêµ­ì–´ í•™ìŠµ ëª¨ë¸ì´ ìœ ë¦¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """)
        
        st.download_button("ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", df.to_csv().encode('utf-8'), "lmm_arena_results.csv")